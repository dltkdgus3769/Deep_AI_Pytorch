{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T01:17:21.310294Z",
     "start_time": "2025-03-21T01:15:31.664513Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "# âœ… í•œêµ­ì–´ ë¬¸ì¥ ì˜ˆì œ ë°ì´í„°ì…‹\n",
    "corpus = [\n",
    "    \"ë‚˜ëŠ” ë„ˆë¥¼ ì‚¬ë‘í•´\",\n",
    "    \"ë‚˜ëŠ” ì½”ë”©ì„ ì¢‹ì•„í•´\",\n",
    "    \"ë„ˆëŠ” ë‚˜ë¥¼ ì¢‹ì•„í•´\",\n",
    "    \"ë„ˆëŠ” íŒŒì´ì¬ì„ ê³µë¶€í•´\",\n",
    "    \"ìš°ë¦¬ëŠ” ì¸ê³µì§€ëŠ¥ì„ ì—°êµ¬í•´\",\n",
    "    \"ë”¥ëŸ¬ë‹ì€ ì¬ë¯¸ìˆì–´\",\n",
    "    \"íŒŒì´ì¬ì€ ê°•ë ¥í•´\",\n",
    "    \"ë‚˜ëŠ” ìì—°ì–´ì²˜ë¦¬ë¥¼ ê³µë¶€í•´\",\n",
    "]\n",
    "\n",
    "# âœ… í˜„ì¬ ë””ë ‰í† ë¦¬ì˜ JSON íŒŒì¼ëª…\n",
    "json_file = \"ì½”ë¡œë‚˜_naver_news.json\"\n",
    "\n",
    "# âœ… ì •ê·œì‹: í•œê¸€ê³¼ ê³µë°±ë§Œ ì¶”ì¶œ\n",
    "hangul_pattern = re.compile(r\"[ê°€-í£\\s]+\")\n",
    "\n",
    "# âœ… corpusì— JSONì˜ title í•œê¸€ ë¬¸ì¥ ì¶”ê°€\n",
    "if os.path.exists(json_file):\n",
    "    with open(json_file, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    for item in data:\n",
    "        if isinstance(item, dict) and \"title\" in item:\n",
    "            title = item[\"title\"]\n",
    "            if isinstance(title, str):\n",
    "                cleaned = \"\".join(hangul_pattern.findall(title)).strip()\n",
    "                if cleaned:\n",
    "                    corpus.append(cleaned)\n",
    "\n",
    "    print(f\"âœ… 'title' í‚¤ì—ì„œ í•œê¸€ ë¬¸ì¥ {len(corpus)}ê°œê°€ corpusì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "else:\n",
    "    print(\"âŒ JSON íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "\n",
    "\n",
    "# âœ… ë‹¨ì–´ ì‚¬ì „ ë§Œë“¤ê¸°\n",
    "word_list = list(set(\" \".join(corpus).split()))\n",
    "word_dict = {w: i for i, w in enumerate(word_list)}\n",
    "idx_dict = {i: w for w, i in word_dict.items()}\n",
    "\n",
    "# âœ… ë°ì´í„°ì…‹ ë³€í™˜\n",
    "def make_data(corpus):\n",
    "    inputs, targets = [], []\n",
    "    for sentence in corpus:\n",
    "        words = sentence.split()\n",
    "        for i in range(len(words) - 1):  # \"ë‚˜ëŠ” ë„ˆë¥¼\" -> \"ì‚¬ë‘í•´\"\n",
    "            x = [word_dict[w] for w in words[:i+1]]\n",
    "            y = word_dict[words[i+1]]\n",
    "            inputs.append(x)\n",
    "            targets.append(y)\n",
    "\n",
    "    return inputs, targets\n",
    "\n",
    "inputs, targets = make_data(corpus)\n",
    "\n",
    "# âœ… íŒ¨ë”© ì¶”ê°€ (ë¬¸ì¥ ê¸¸ì´ë¥¼ ë§ì¶¤)\n",
    "max_len = max(len(seq) for seq in inputs)\n",
    "inputs_padded = [seq + [0] * (max_len - len(seq)) for seq in inputs]\n",
    "targets = torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "# âœ… ë°ì´í„°ì…‹ ë° DataLoader ìƒì„±\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = torch.tensor(inputs, dtype=torch.long)\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.targets[idx]\n",
    "\n",
    "dataset = TextDataset(inputs_padded, targets)\n",
    "train_loader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "vocab_size = len(word_dict)  # ë‹¨ì–´ ê°œìˆ˜\n",
    "embed_size = 10  # ì„ë² ë”© ì°¨ì›\n",
    "hidden_size = 16  # RNN ì€ë‹‰ì¸µ í¬ê¸°\n",
    "num_classes = len(word_dict)  # ì˜ˆì¸¡í•  ë‹¨ì–´ ê°œìˆ˜\n",
    "\n",
    "\n",
    "# 2ï¸ RNN ëª¨ë¸ ì •ì˜\n",
    "class RNNTextModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_classes):\n",
    "        super(RNNTextModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)  # ë‹¨ì–´ ì„ë² ë”©\n",
    "        self.rnn = nn.RNN(embed_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        out, _ = self.rnn(x)\n",
    "        out = self.fc(out[:, -1, :])  # ë§ˆì§€ë§‰ ì‹œì ì˜ RNN ì¶œë ¥ì„ ì‚¬ìš©\n",
    "        return out\n",
    "\n",
    "# âœ… ëª¨ë¸ ìƒì„±\n",
    "model = RNNTextModel(vocab_size, embed_size, hidden_size, num_classes)\n",
    "\n",
    "# âœ… GPU ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ ì´ë™\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# âœ… ì†ì‹¤ í•¨ìˆ˜ ë° ìµœì í™” í•¨ìˆ˜ ì„¤ì •\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "# 3ï¸ ëª¨ë¸ í•™ìŠµ ë° ì €ì¥\n",
    "num_epochs = 100\n",
    "print(\"ğŸš€ RNN ëª¨ë¸ í•™ìŠµ ì‹œì‘...\")\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# âœ… ëª¨ë¸ ì €ì¥\n",
    "model_path = \"./rnn_news_model.pth\"\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"âœ… í•™ìŠµëœ ëª¨ë¸ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {model_path}\")\n",
    "# 4ï¸ ì €ì¥ëœ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "\n",
    "\n",
    "\n",
    "# âœ… ì €ì¥ëœ RNN ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "def load_model(model_path, vocab_size, embed_size, hidden_size, num_classes):\n",
    "    model = RNNTextModel(vocab_size, embed_size, hidden_size, num_classes)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=torch.device(\"cpu\")))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# âœ… ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "loaded_model = load_model(model_path, vocab_size, embed_size, hidden_size, num_classes)\n",
    "print(\"âœ… ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™€ì¡ŒìŠµë‹ˆë‹¤!\")\n",
    "# 5ï¸ ìƒ˜í”Œ ë¬¸ì¥ ì˜ˆì¸¡ (ì˜ˆì¸¡ ë‹¨ì–´ ë° ì •í™•ë„ ì¶œë ¥)\n",
    "\n",
    "\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def predict_next_word(model, sentence):\n",
    "    \"\"\"\n",
    "    ì €ì¥ëœ RNN ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì£¼ì–´ì§„ ë¬¸ì¥ì˜ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” í•¨ìˆ˜.\n",
    "    \"\"\"\n",
    "    # âœ… ì…ë ¥ ë¬¸ì¥ì„ ì •ìˆ˜ ì¸ì½”ë”©\n",
    "    words = sentence.split()\n",
    "    input_seq = [word_dict[w] for w in words if w in word_dict]\n",
    "\n",
    "    # âœ… íŒ¨ë”© ì¶”ê°€ (ê¸¸ì´ë¥¼ ë§ì¶”ê¸° ìœ„í•´)\n",
    "    input_padded = input_seq + [0] * (max_len - len(input_seq))\n",
    "    input_tensor = torch.tensor([input_padded], dtype=torch.long)\n",
    "\n",
    "    # âœ… ëª¨ë¸ ì˜ˆì¸¡\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        probabilities = F.softmax(output[0], dim=0)\n",
    "        predicted_idx = torch.argmax(probabilities).item()\n",
    "        confidence = probabilities[predicted_idx].item()\n",
    "\n",
    "    predicted_word = idx_dict[predicted_idx]\n",
    "\n",
    "    print(f\"ğŸ” ì…ë ¥ ë¬¸ì¥: '{sentence}'\")\n",
    "    print(f\"ğŸ“Š ì˜ˆì¸¡ëœ ë‹¨ì–´: '{predicted_word}'\")\n",
    "    print(f\"âœ… ì˜ˆì¸¡ í™•ë¥ : {confidence * 100:.2f}%\")\n",
    "\n",
    "# ğŸ† ìƒ˜í”Œ ë¬¸ì¥ ì˜ˆì¸¡ ì‹¤í–‰\n",
    "sample_sentence = \"ë‚˜ëŠ” ë„ˆë¥¼\"\n",
    "predict_next_word(loaded_model, sample_sentence)"
   ],
   "id": "beef8f6de109e258",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… 'title' í‚¤ì—ì„œ í•œê¸€ ë¬¸ì¥ 808ê°œê°€ corpusì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "ğŸš€ RNN ëª¨ë¸ í•™ìŠµ ì‹œì‘...\n",
      "Epoch [10/100], Loss: 7.6568\n",
      "Epoch [20/100], Loss: 6.7330\n",
      "Epoch [30/100], Loss: 6.0405\n",
      "Epoch [40/100], Loss: 7.9360\n",
      "Epoch [50/100], Loss: 8.6157\n",
      "Epoch [60/100], Loss: 8.4986\n",
      "Epoch [70/100], Loss: 8.5448\n",
      "Epoch [80/100], Loss: 8.5733\n",
      "Epoch [90/100], Loss: 8.5219\n",
      "Epoch [100/100], Loss: 8.6236\n",
      "âœ… í•™ìŠµëœ ëª¨ë¸ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: ./rnn_news_model.pth\n",
      "âœ… ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™€ì¡ŒìŠµë‹ˆë‹¤!\n",
      "ğŸ” ì…ë ¥ ë¬¸ì¥: 'ë‚˜ëŠ” ë„ˆë¥¼'\n",
      "ğŸ“Š ì˜ˆì¸¡ëœ ë‹¨ì–´: 'ì½”ë¡œë‚˜'\n",
      "âœ… ì˜ˆì¸¡ í™•ë¥ : 3.22%\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T02:29:33.542436Z",
     "start_time": "2025-03-21T02:29:33.539845Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ğŸ† ìƒ˜í”Œ ë¬¸ì¥ ì˜ˆì¸¡ ì‹¤í–‰\n",
    "sample_sentence = \"ì½”ë¡œë‚˜ \"\n",
    "predict_next_word(loaded_model, sample_sentence)"
   ],
   "id": "7a5e4e41d0bc16a5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” ì…ë ¥ ë¬¸ì¥: 'ì½”ë¡œë‚˜ '\n",
      "ğŸ“Š ì˜ˆì¸¡ëœ ë‹¨ì–´: 'ì½”ë¡œë‚˜'\n",
      "âœ… ì˜ˆì¸¡ í™•ë¥ : 3.50%\n"
     ]
    }
   ],
   "execution_count": 18
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
